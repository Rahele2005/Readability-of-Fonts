{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"**Comparing Readability and Eye Movement Patterns Between Uppercase and Lowercase Fonts in English: An Eye-Tracking Study**\"\n",
        "\n",
        "authors:\n",
        "  - name: Sveva Battisti (3756089)\n",
        "    affiliation: University of Stuttgart\n",
        "    affiliation-email: st191420@stud.uni-stuttgart.de\n",
        "  - name: Raheleh Soltani (3635045)\n",
        "    affiliation: University of Stuttgart\n",
        "    affiliation-email: st182064@stud.uni-stuttgart.de\n",
        "  - name: Aishwarya Pandurang (3643749)\n",
        "    affiliation: University of Stuttgart\n",
        "    affiliation-email: st185663@stud.uni-stuttgart.de\n",
        "date-format: long\n",
        "date: last-modified\n",
        "\n",
        "toc: true \n",
        "lof: true\n",
        "format:\n",
        "  pdf:\n",
        "    titlepage: true\n",
        "    titlepage-footer: |\n",
        "        **Final report**\\\n",
        "        Acquisition and Analysis of \\\n",
        "        Eye Tracking Data\\\n",
        "    \n",
        "    titlepage-theme:\n",
        "      elements: [\"\\\\titleblock\", \"\\\\authorblock\", \"\\\\vfill\", \"\\\\today \\\\vspace{0.8cm}\",\"\\\\footerblock\", ]\n",
        "      page-align: \"center\"\n",
        "      title-style: \"plain\"\n",
        "      title-fontstyle: [\"Huge\", \"textbf\"]\n",
        "      title-space-after: \"1.5cm\"\n",
        "      subtitle-fontstyle: \"large\"\n",
        "      title-subtitle-space-between: \"0.5cm\"\n",
        "      author-style: \"plain\"\n",
        "      author-sep: \"newline\"\n",
        "      author-fontstyle: \"textbf\"\n",
        "      author-space-after: \"2\\\\baselineskip\"\n",
        "      affiliation-style: \"numbered-list-with-correspondence\"\n",
        "      affiliation-fontstyle: \"large\"\n",
        "      affiliation-space-after: \"0pt\"\n",
        "      footer-style: \"plain\"\n",
        "      footer-fontstyle: [\"Large\", \"textsc\"]\n",
        "      footer-space-after: \"1cm\"\n",
        "      \n",
        "    toc: true\n",
        "    fontsize: 12pt\n",
        "    number-sections: true\n",
        "    toc-title: \"Table of Contents\"\n",
        "    highlight-style: github\n",
        "    documentclass: report\n",
        "    geometry:\n",
        "      - top=30mm\n",
        "      - left=20mm\n",
        "      - right=20mm\n",
        "      - heightrounded\n",
        "    colorlinks: true\n",
        "    cite-method: biblatex\n",
        "    include-in-header: \n",
        "        text: |\n",
        "            \\usepackage{fvextra}\n",
        "            \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "            \\usepackage{csquotes}\n",
        "         \n",
        "    \n",
        "    monofontoptions: \n",
        "        - Scale=0.80\n",
        "  html:\n",
        "    code-fold: true\n",
        "bibliography: biblio.bib\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# abstract: \n",
        "\n",
        "This study investigates the readability of uppercase versus lowercase fonts in English sentences through an eye-tracking experiment. We aimed to determine whether uppercase text is read more slowly than lowercase text by L2 speakers of English. Ten participants were tested on various sentences presented in both uppercase and lowercase fonts. Key metrics, including the number of fixations, fixation duration, and landing spot, were recorded and analysed. Our results provide insights into how font case affects reading efficiency and eye movement patterns, contributing to the broader understanding of typographic design and readability.\n",
        "# Introduction\n",
        "\n",
        "## Readability of Fonts, UPPERCASE vs lowercase\n",
        "\n",
        "Eye tracking technology has a wide range of applications in research on visual system, psychology, psycholinguistics and human-computer interaction, among other fields. Different types of eye trackers are available, varying in precision and suitability depending on the specific goals of the experiment. These tools, coupled with advanced data processing techniques, allow researchers to gather detailed information about visual and cognitive processes.\n",
        "In the field of linguistics, eye tracking has become increasingly common as it provides a precise method for determining where attentional resources are allocated during language processing tasks. By directly measuring cognitive processing effort, eye tracking helps researchers understand how the brain engages with and interprets language. \n",
        "This study focuses on the readability of uppercase versus lowercase fonts in English sentences, using eye tracking to assess whether uppercase text is read more slowly than lowercase text. The motivation for our study is deeply rooted in the seminal findings of Tinker and Paterson (1939). Their pioneering research, utilizing the photographic technique, revealed that reading uppercase text resulted in a 7% increase in reading time compared to lowercase text, accompanied by a greater number of fixations. Intriguingly, despite the increased number of fixations, the duration of these fixations was 20 milliseconds shorter for uppercase text. In contrast, a later study by White and Liversedge (2006), which employed a different experimental design and aimed to explore the topic from another angle, observed a slight decrease in reading time for uppercase text compared to lowercase, amounting to a mere 2%. Their findings indicated no significant difference in the number of fixations between the two font cases. These contrasting results underscore the complexity of reading processes and the influence of text case on reading dynamics, providing a rich foundation for our current investigation. Additionally, White and Liversedge’s findings on the landing spot- a key focus of our study- revealed that readers tended to land closer to the beginning of words when reading uppercase text. This particular insight has become a cornerstone for our investigation, guiding our primary objective to explore how text case influences reading behaviour. While Tinker and Paterson’s study definitely demonstrates that case type significantly impacts reading behaviour, with uppercase text proving more difficult to read than lowercase, the findings of White and Liversedge presented a different narrative. Their study indicated only limited difference in eye movement behaviour when participants read uppercase text compared to lowercase. This contrast in results highlights the nuanced and complex nature of how text case influences reading dynamics, further justifying our current exploration into this intriguing area. \n",
        "In the same vein, we aim to explore how typographic variations impact reading efficiency by analysing key metrics such as the number of fixations, fixation duration, and landing spots. Understanding these factors contributes to our knowledge of effective typographic design and its implications for readability. The experiment involves ten participants who read sentences presented in both uppercase and lowercase fonts. Through this investigation, we hope to shed light on the cognitive and visual mechanisms underlying text processing, ultimately informing better design choices for written materials. \n",
        "One important factor in understanding comprehension and readability of fonts is the preferred location on words where the eyes fixate. Kerr and Zola (1988) propose five principles of perceptuo-oculomotor control to explain why the landing spot is often to the left side of the word’s centre. This specific location appears to facilitate easier word comprehension. According to Kerr and Zola, readers develop the ability to adjust their gaze to position their eyes optimally. The convenient viewing hypothesis suggests that forward saccades can be influenced by the position of the eyes relative to the word. This highlights the word-based nature of eye guidance. Essentially, readers fine-tune their oculomotor behaviour to achieve optimal performance, taking into account the constraints of the visual system and the properties of the text stimuli.\n",
        "Results from O'Regan and Jacobs (1992) showed distance of landing spot from optimal position, which they believe to be near the middle or just left of the middle of words, does impact lexical decision time. In the same vein, it is logical to consider how differences in font case, and consequently the landing spot, influence readability. The authors suggest a connection between the strength of optimal viewing point and the hypothesis that there is a fall-off in acuity within the fovea. It can be argued that the inconsistency of the landing spot in lower and uppercase fonts might be related to peripheral, rather than foveal, acuity in uppercase font.\n",
        "\n",
        "Our research question is: \n",
        "\n",
        "\\emph{In alignment with the results of White and Liversedge, our final hypothesis points that there is no significant difference in reading behaviour between uppercase and lowercase text. We aim to confirm their findings, suggesting that the case of the text does not markedly affect eye movement and overall reading dynamic.}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Experimental Methods\n",
        "\n",
        "\n",
        "Ten L2 (second language) speakers of English participated in this experiment, chosen due to the better availability of L2 speakers in Germany compared to native speakers, and to compare their results with those of native speakers. They were English as a second language speakers with competency of A2 or above. They come from different language backgrounds, such as German, Azeri, Hindu, Italian etc. The participants age was between 20-30. Each participant was provided with two easy-to-comprehend dialogues, presented either in uppercase or lowercase.\n",
        "\n",
        "The dialogues were displayed to each participant in either uppercase or lowercase format through four loops generated on Open Sesame. The sentences of each dialogues appeared one at a time on the screen, and participants could proceed to the next sentence by pressing the space key. Before each line of the dialogue, a fixation dot appeared on a sketch pad on the left side of the screen, cantered vertically. After each sentence, a fixation dot appeared on the right side of the screen. Fixation dots disappeared upon gaze fixation of the participant, and they were then able to observe the next pad. Each trial, or dialogue, lasts about 5 minutes. \n",
        "\n",
        "Participants were asked to answer two comprehension questions after reading each dialogue. They responded by pressing the up key for ‘yes’ and the down key for ‘no’. A logger recorded the onset and another one for the end of each dialogue to track reading times and responses. \n",
        "\n",
        "Each participant read each dialogue either in uppercase or lowercase format, facilitated by the four loops in Open Sesame. The design ensured that the presentation format varied within participants to control for individual reading speed and comprehension differences. This way the dialogues were both counterbalanced and randomized in terms of case. \n",
        "\n",
        "\n",
        "\n",
        "# Experiment Design\n",
        "\n",
        "## Software and Hardware\n",
        "\n",
        "- **Software**: The experiment is designed in [OpenSesame](https://osdoc.cogsci.nl/)[@mathot_opensesame] which is a graphical experiment building software to create experiments for psychology, neuroscience, and experimental economics. Eye trackers can be integrated with OpenSesame to record the eye movements of the participants, which is finally used to analyze the data. It is available on Windows, Mac OS and Linux.\n",
        "\n",
        "- **Language**: Python was used along with the OpenSesame to create the experiment. Libraries like [Pandas](https://pandas.pydata.org/docs/user_guide/index.html), [Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html), [Matplotlib](https://matplotlib.org/stable/users/index.html) were used to analyze the data. Psychopy is used on Open Sesame. Other options available for backends are pyGame, Expyriment, etc.\n",
        "\n",
        "- **Eye Tracker**: The experiment is conducted using the GazePoint GP3 eye tracker. It is a binocular eye tracker that can record at 150 Hz. The eye tracker is connected to the computer and the participants are seated at a distance of around 60 cm from the screen. The experiment was conducted in a dimly lit laboratory setup to avoid any external light source that might interfere with the eye tracking. The GazePoint API [@gazepoint_api] is referred for the anaylsis of the eye tracking data.\n",
        "\n",
        "## Eye Tracker Calibration\n",
        "\n",
        "The GazePoint GP3 eye tracker requires accurate calibration for reliable data. Calibration was conducted before each participant's session, adjusting for factors such as glasses, contact lenses, and individual participant differences. To ensure data quality, calibration was repeated if the tracker indicated significant deviations. For participants unable to calibrate successfully, additional measures included adjusting the tracker’s settings and providing multiple calibration attempts.\n",
        "\n",
        "\n",
        "## Structure of the experiment\n",
        "\n",
        "The main idea of this experiment was to investigate whether font types impact eye movement patterns, such as fixation and duration, and consequently affect overall readability. We primarily came up with the idea that font types that promote clear letter shapes and spacing would reduce the occurrence of fixating eye movements, thus enhancing readability.\n",
        "\n",
        "To test this, we used font format (uppercase and lowercase) as independent variables, and fixation duration, reading speed, comprehension, and regressive eye movements (number and duration) as the dependent variables. We also decided to measure the landing spot, which we reckoned to serve as a benchmark for measuring reading time. \n",
        "\n",
        "The conditions were tested using within-subjects paradigm, meaning each participant was exposed to both uppercase and lowercase formats. We set up the experiment on Open Sesame, which, through iterative development over the semester, was refined to better meet our goals. \n",
        "\n",
        "In the experiment, participants first saw a fixation dot, followed by sentences of a dialogue presented one at a time, and then comprehension questions. We collected data on the number of fixations, fixation durations, and the coordinates of each word on the screen to create bounding boxes for each word. Employing these bounding boxes, showing the position of each word and the sentence, facilitated by a monospaced font, we could accurately measure the landing site. \n",
        "\n",
        "Using the pilot data, we generated some graphs for sanity check. One of the graphs we used plotted time against x-position, with added vertical lines to check the triggers/user logs. This allowed us to determine how engaged the participant was throughout the experiment and to identify specific points of interest or disengagement. \n",
        "\n",
        "##############\n",
        "The following @fig-ui shows the best point of gaze in x-coordinate versus time in two of the participants:\n",
        "\n",
        "![best point of gaze in x-coordinate](plot/timevsBPOGX.png){#fig-ui width=400}\n",
        "\n",
        "The other graph plotted the best point of gaze in x-coordinate against that of y-coordinate, where we observed the fixation spread, outliers etc. \n",
        "\n",
        "![best point of gaze in x-coordinate against y-coordinate](plot/BPOGXvsBPOGY.png){#fig-ui width=400}\n",
        "###############\n",
        "\n",
        "Since we believed that if one type of font case might require more fixations and longer fixation durations to read, we visualized both the number of fixations and the fixation durations. The rationale for analysing fixation durations is that the length of fixations might be influenced by the distinct character shapes in the two font cases. Our preliminary observation illustrates that one participant (subject 2) had more fixations in the lowercase segments compared to the uppercase segments. Similarly, subject 3 exhibited more eye movements when reading lowercase text, suggesting that processing lowercase font might be more effortful. \n",
        "\n",
        "Despite observing more fixations in lowercase segment, the fixation durations for the two subjects in our preliminary experiment did not show much difference between the two font cases. The variations in landing spots further suggest that initial eye movements differ between uppercase and lowercase text. This indicates that different font types influence how eyes are moved during reading, leading to various processing behaviours. \n",
        "\n",
        "These opposing results motivated us to strengthen our findings by running the experiment on more subjects.\n",
        "\n",
        "## Design of the Experiment\n",
        "\n",
        "The experiment follows the following structure in OpenSesame:\n",
        "\n",
        "1. **Introduction to the experiment**: It contains some preliminary instructions for the participants to understand the experiment. It also mentions that each progression will require a key press. The foreground color is set to black and the background color is set to white throughout the experiment.\n",
        "\n",
        "\n",
        "\n",
        "2. **Initialization of variables**: The position variables (center) is initialized and is used to set the position of the sentence in the grid. The `pygaze` module is also intialized to record the eye movements of the participants.\n",
        "\n",
        "3. **Trial Loop Items**: This loop runs the experiment for 2 trials. The trial loop contains the following sequence of events:\n",
        "    - **First Fixation Dot**: A fixation cross is displayed at the left center of the screen. It is a black dot on a white background which is displayed to ensure that the participants are looking at the centeral left side of the screen before the sentence appears. It is displayed using the sketchpad item in OpenSesame.\n",
        "    \n",
        "    - **Stimulus**: \n",
        "        The stimuli that is a sentence, either in lowercase or uppercase, will appear. \n",
        "        Participants will see one sentence at a time.\n",
        "        There are two dialogues of analogous difficulty and the same number of words in each line, i.e. 10.\n",
        "        Each of the two dialogues will be seen in either uppercase and lowercase.\n",
        "        Each participant will see two dialogues.\n",
        "        Each response is captured with a mouse click. The mouse click is recorded and logged using the `mouse_click_response` item in OpenSesame.\n",
        " \n",
        "    - **Second Fixation Dot**: A second fixation dot appears after each sentence on a new sketchpad. It is a black dot located in the center and right side of the screen.\n",
        "    \n",
        "    - **Logging**: The onset and offset of the fixation instruction and the stimulus (sentence) are logged for each trial. Additionally, there is loggings for the fixation dot, case type, and prior and post eye-tracker loggings.\n",
        "    \n",
        "    \n",
        "\n",
        "4. **End of Experiment**: The experiment ends with a thank you message for the participants.\n",
        "\n",
        "The timeline of one trial is shown in @fig-trial-timeline .\n",
        "\n",
        "![Timeline of a trial](plots/trial_timeline.png){#fig-trial-timeline width=90%}\n",
        "\n",
        "###############\n",
        "\n",
        "## Logic of the experiment\n",
        "\n",
        "1. Stimuli are chosen as per the discussions and consultations throughout the semester. \n",
        "    - For e.g., One criteria of choosing these senetences is equality in the length of each sentence and degree of difficulty of dialogues.\n",
        "\n",
        "2. A fixation at any point on the screen indicates that the participant is paying attention to it. Thus, we record the fixations throughout the experiment to deduce the attention of the pariticipant when we instruct them to fixate at a certain point of the canvas.\n",
        "\n",
        "\n",
        "3. Fixations, at the centre left side before the sentence and at the center right side after the sentence, on the screen mark the start and end of each sentence as stimuli units. It will also helps the participants anticipate/expect the next sentence and be ready to cognitively relate and comprehend them.\n",
        "\n",
        "\n",
        "\n",
        "## Stimuli Preprocessing\n",
        "\n",
        "The stimuli collected needed to be randomized, in loops, and preprocessed before they could be employed in the experiment. The preprocessing steps are as follows:\n",
        "\n",
        "1. \n",
        "      \n",
        " ```import random\n",
        "case_order = random.choice ([0, 1])\n",
        "\n",
        "var.case_order=case_order\n",
        "print(\"Case order ...\",var.case_order)\n",
        "if var.case_order == 1:\n",
        "   no=random.choice([0,1])\n",
        "   if no==1:\n",
        "       exp.items['lowercase_loop'].run()\n",
        "       exp.items['lowercase_questions_introduction'].run()\n",
        "       exp.items['comprehension_questions_loop_lowercase'].run()\n",
        "       exp.items['exit_sketchpad'].run()\n",
        "       exp.items['uppercase_loop'].run()\n",
        "       exp.items['uppercase_questions_introduction'].run()\n",
        "       exp.items['comprehension_questions_loop_uppercase'].run()\n",
        "   else:\n",
        "       exp.items['lowercase_loop1'].run()\n",
        "       exp.items['lowercase_questions_introduction'].run()\n",
        "       exp.items['comprehension_questions_loop_lowercase1'].run()\n",
        "       exp.items['exit_sketchpad'].run()\n",
        "       exp.items['uppercase_loop1'].run()\n",
        "       exp.items['uppercase_questions_introduction'].run()\n",
        "       exp.items['comprehension_questions_loop_uppercase1'].run()\n",
        "   \n",
        "   \n",
        "  \n",
        "else:\n",
        "    no=random.choice([0,1])\n",
        "    if no==1:\n",
        "        exp.items['uppercase_loop'].run()\n",
        "        exp.items['uppercase_questions_introduction'].run()\n",
        "        exp.items['comprehension_questions_loop_uppercase'].run()\n",
        "        exp.items['exit_sketchpad'].run()\n",
        "        exp.items['lowercase_loop'].run()\n",
        "        exp.items['lowercase_questions_introduction'].run()\n",
        "        exp.items['comprehension_questions_loop_lowercase'].run()\n",
        "    else:\n",
        "        exp.items['uppercase_loop1'].run()\n",
        "        exp.items['uppercase_questions_introduction'].run()\n",
        "        exp.items['comprehension_questions_loop_uppercase1'].run()\n",
        "        exp.items['exit_sketchpad'].run()\n",
        "        exp.items['lowercase_loop1'].run()\n",
        "        exp.items['lowercase_questions_introduction'].run()\n",
        "        exp.items['comprehension_questions_loop_lowercase1'].run()\n",
        "        \n",
        "    ```\n",
        "\n",
        "2. left side fixation dot had to be logged using the following code.\n",
        "```\n",
        "    ```screen_width = 1920\n",
        "screen_height = 1080\n",
        "left_centre = (screen_width / 4, screen_height / 2)  # Left center of the screen\n",
        "\n",
        "central_fixation = False\n",
        "while not central_fixation:\n",
        "    # Get a current (x,y) sample from the tracker\n",
        "    gazepos = eyetracker.sample()  # Replace with the actual method to get gaze position\n",
        "\n",
        "    # Check distance between the sample and the left center\n",
        "    d = math.sqrt((gazepos[0] - left_centre[0])**2 + (gazepos[1] - left_centre[1])**2)\n",
        "    \n",
        "    # Check if distance is \"small enough\"\n",
        "    if d < 100:    \n",
        "        central_fixation = True\n",
        "        \n",
        "    # Check for a timeout (10 seconds)\n",
        "    if time.time() - t0 > 10000:  # 10 seconds\n",
        "        break\n",
        "    ```\n",
        "screen resolution was modified to match the one in the lab.\n",
        "\n",
        "3. Using math and time libraries, the right side fixation dot was set. \n",
        "```\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Function to get the gaze position from the eye-tracker\n",
        "def get_gaze_position():\n",
        "    # Replace with the actual method to get the current gaze position from your eye-tracker\n",
        "    return eyetracker.sample()\n",
        "\n",
        "# Get the start time\n",
        "t0 = time.time()\n",
        "\n",
        "# Compute the right center coordinates\n",
        "screen_width = 1920\n",
        "screen_height = 1080\n",
        "right_centre = (3 * screen_width / 4, screen_height / 2)  # Right center of the screen\n",
        "\n",
        "central_fixation = False\n",
        "while not central_fixation:\n",
        "    # Get a current (x, y) sample from the tracker\n",
        "    gazepos = get_gaze_position()  # Replace with the actual method to get gaze position\n",
        "\n",
        "    # Check distance between the sample and the right center\n",
        "    d = math.sqrt((gazepos[0] - right_centre[0])**2 + (gazepos[1] - right_centre[1])**2)\n",
        "    \n",
        "    # Check if distance is \"small enough\"\n",
        "    if d < 100:    \n",
        "        central_fixation = True\n",
        "        \n",
        "    # Check for a timeout (10 seconds)\n",
        "    if time.time() - t0 > 10:  # 10 seconds\n",
        "        break\n",
        "   ```\n",
        "The function helps detect the gaze before the sentence appears to prevent odds of outliers in fixations.\n",
        "\n",
        "4. The landing spot on one of the words in each sentence was detected using the following code. \n"
      ],
      "id": "82cffed0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load eye-tracking data from CSV\n",
        "df = pd.read_csv(\"subject-1.csv\")\n",
        "\n",
        "# Define the sentence and its words\n",
        "sentence = \"HI, SARAH! HOW WAS YOUR WEEKEND?\"\n",
        "words = sentence.split()\n",
        "\n",
        "# Bounding boxes for words in the sentence (example coordinates)\n",
        "words_bounding_boxes = {\n",
        "    'HI,': (-288, 0, -192, 32),\n",
        "    'SARAH!': (-192, 0, 0, 32),\n",
        "    'HOW': (0, 0, 96, 32),\n",
        "    'WAS': (96, 0, 224, 32),\n",
        "    'YOUR': (224, 0, 352, 32),\n",
        "    'WEEKEND?': (352, 0, 576, 32)\n",
        "}\n",
        "\n",
        "# Function to convert normalized coordinates to pixel values\n",
        "def normalized_to_pixel(norm_x, norm_y, screen_width, screen_height):\n",
        "    return norm_x * screen_width, norm_y * screen_height\n",
        "\n",
        "# Function to plot sentence and fixations\n",
        "def plot_sentence_fixations(sentence, fixations):\n",
        "    fig, ax = plt.subplots(figsize=(12, 2))  # Adjust figsize as needed\n",
        "\n",
        "    # Plot sentence\n",
        "    ax.text(0.5, 0.5, sentence, va='center', ha='center', fontsize=12, color='black')\n",
        "\n",
        "    # Plot fixations\n",
        "    for index, row in fixations.iterrows():\n",
        "        x, y = normalized_to_pixel(row['BPOGX'], row['BPOGY'], ax.get_xlim()[1], ax.get_ylim()[1])\n",
        "        ax.plot(x, y, marker='o', markersize=8, color='red', label='Fixation')\n",
        "\n",
        "    ax.set_xlim(-300, 600)  # Adjust limits based on sentence layout\n",
        "    ax.set_ylim(-10, 50)    # Adjust limits based on sentence layout\n",
        "    ax.set_axis_off()       # Turn off axis\n",
        "\n",
        "    plt.title(\"Sentence with Fixations\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Assuming fixations are already extracted from df (as in previous examples)\n",
        "fixation_word_df = df[['BPOGX', 'BPOGY']]  # Assuming df contains FPOGX and FPOGY columns\n",
        "\n",
        "plot_sentence_fixations(sentence, fixation_word_df)"
      ],
      "id": "94e1b35c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Eye tracker calibration**:\n",
        "    - **Challenge**: The eye tracker calibration was a challenge as the participants were not able to calibrate with the eye tracker, due to many reasons like contact lenses, glasses, height of the participants, body posture, etc., during the experiment.\n",
        "    - **Solution**: The number of participants were increased to more than 10 to ensure that we have atleast 9-10 participants with proper calibration and the experiment was run a few times before the participant starts.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "1. Since the stimuli units are sentences, and each sentence must be preceded and followed by a fixation dot, the odds of forgetting the content of the previous sentence could increase.\n",
        "\n",
        "2. Answering the two follow-up comprehension questions could have been disracting and one was prone to changing the position of head and hence deficient data, even when chin rest was utilized.\n",
        "\n",
        "3. Competency of participants in English differed and this could be the reason for variability in reading behaviours, rather than the experimental items.\n",
        "\n",
        "4. The number of words in each sentence are equal, despite this, the general difficulty and length of the words and the two dialogues were not easy to control.\n",
        "\n",
        "5. The bounding boxes that we found to find the landing spots were too high and also the finding the landing spot on single words was not easy to resolve.\n",
        "\n",
        "# Further Improvements\n",
        "\n",
        "This projects although replicates the results of the reference paper to a great extent, there are still some improvements that can make the results and the analysis more robust and reliable. Some of the improvements are as follows:\n",
        "\n",
        "1. More extensive study with a larger number of participants will ensure that the results are more generalizable. The study can be conducted with participants from different age groups and educational backgrounds. Also the study can be conducted in different languages to prove the universality of the observed effects.\n",
        "\n",
        "2. The Gazepoint GP3 eye tracker can be replaced with a more advanced eye tracker like Eyelink 1000 which can record at a higher sampling rate and thus provide more accurate results. The Eyelink 1000 eye tracker allows less head movement due to its design and thus the recording of the eye movements are more accurate.\n",
        "\n",
        "3. The experiment requires us to study the fixation behavior of the participants. Therefore, better optimized fixation detection algorithms can be used and even integrated to the open source software like OpenSesame to improve the analysis.\n",
        "\n",
        "## Results Summary\n",
        "\n",
        "The analysis of eye-tracking data revealed that uppercase text resulted in a marginally higher number of fixations compared to lowercase text, aligning with Tinker and Paterson’s findings. However, the duration of fixations and landing spots did not show significant differences, consistent with White and Liversedge’s results. Statistical analysis confirmed that these variations were not statistically significant, supporting our hypothesis that font case does not markedly affect reading behavior.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "In this project we attempted to investigate reading behaviours of participants in regards with two font case formats. Our final research hypothesis, in alignment with the results of White and Liversedge, posits that there is no significant difference in reading behavior between uppercase and lowercase text. This finding supports previous research indicating limited differences in eye movement behavior when participants read text in different cases [@white2006]. Similarly, Tinker and Paterson’s earlier work highlighted the nuances of text readability in different cases, further emphasizing the complexity of reading processes [@tinker1939].\n",
        "\n",
        "By confirming these findings, our study contributes to the ongoing discourse on typography and reading efficiency, providing valuable insights for future research.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Contribution Table\n",
        "\n",
        "| **Task**                                 | **Sveva** | **Raheleh** | **Aishwarya** |\n",
        "|------------------------------------------|:----------:|:---------:|:---------:|\n",
        "| Background Literature                    |      o     |     o     |     x     |\n",
        "| Experiment Design                        |      o     |     o     |     o     |\n",
        "| Stimulus Design                          |      o     |     o     |     o     |\n",
        "| Piloting                                 |      o     |     o     |     o     |\n",
        "| Data-Recording                           |      o     |     o     |     o     |\n",
        "| Non-Final-Talk presenting (who talks)    |      o     |     x     |     x     |\n",
        "| Non-Final-Talk presenting (who prepares) |      o     |     x     |     x     |\n",
        "| Final-Talk Presenting (who talks)        |      o     |     o     |     o     |\n",
        "| Final-Talk Presenting (who prepares)     |      o     |     x     |     x     |\n",
        "| Data Analysis Scripts                    |      o     |     x     |     o     |\n",
        "| Report Writing                           |      x     |     o     |     x     |\n",
        "\n",
        ": Contributions of each team member to the project\n",
        "\n",
        "- x: main contributor\n",
        "- o: supporting contributor\n",
        "\n",
        "\n",
        "# References {.unnumbered}"
      ],
      "id": "06719052"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\rahel\\OneDrive\\Desktop\\Readability-of-Fonts\\report\\my-report\\venv\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}